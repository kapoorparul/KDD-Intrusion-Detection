{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import precision_recall_fscore_support as prfs\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file(fname, ratio, trainname, testname):\n",
    "    Nobs = file_len(fname)\n",
    "    tfp = int(Nobs * ratio)\n",
    "    choices_test = np.sort(np.random.choice(Nobs,tfp, replace=False))\n",
    "    k = 0\n",
    "    file = open(fname)\n",
    "    file_tr = open(trainname, 'w')\n",
    "    file_ts = open(testname, 'w')\n",
    "    uniq = []\n",
    "    \n",
    "    for i in range(Nobs):\n",
    "        \n",
    "        line = file.readline()\n",
    "        \n",
    "        var = line.split(',')\n",
    "        size = len(var)\n",
    "        val = var[size-1]\n",
    "        \n",
    "        if(val in uniq):\n",
    "            pass\n",
    "        else:\n",
    "            file_ts.write(line)\n",
    "            uniq.append(val)\n",
    "            \n",
    "        if(k<choices_test.shape[0]):\n",
    "            if(choices_test[k] == i):\n",
    "                file_ts.write(line)\n",
    "                k = k+1\n",
    "            else:\n",
    "                file_tr.write(line)\n",
    "        else:\n",
    "            file_tr.write(line)\n",
    "            \n",
    "    file_ts.close()\n",
    "    file_tr.close()\n",
    "    file.close()\n",
    "    return Nobs, choices_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already splited dont repeat...\n",
    "test_split = False\n",
    "if(test_split):\n",
    "    Nobs, test_size = split_file('kddcup.data.corrected', 0.25, 'train_val','test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = False\n",
    "if(val_split):\n",
    "    Nobs, test_size = split_file('train_val', 0.3333, 'train','val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file names\n",
    "trainfile = 'train'\n",
    "testfile = 'test'\n",
    "valfile = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 2449339 observations\n"
     ]
    }
   ],
   "source": [
    "train_size = file_len(trainfile)\n",
    "print('Training data has',train_size, 'observations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove duplicates and save as npy'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"remove duplicates and save as npy\"\"\"\n",
    "\n",
    "# df = pd.read_csv('train')\n",
    "# df2 = df.drop_duplicates()\n",
    "# df = None\n",
    "# batcharr = df2.values\n",
    "# print(batcharr.shape)\n",
    "# np.save('train_reduced', df2.values)\n",
    "\n",
    "# df = pd.read_csv('test')\n",
    "# df2 = df.drop_duplicates()\n",
    "# df = None\n",
    "# batcharr = df2.values\n",
    "# print(batcharr.shape)\n",
    "# np.save('test_reduced', df2.values)\n",
    "\n",
    "# df = pd.read_csv('val')\n",
    "# df2 = df.drop_duplicates()\n",
    "# df = None\n",
    "# batcharr = df2.values\n",
    "# print(batcharr.shape)\n",
    "# np.save('val_reduced', df2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To save entire test data as npy file'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''To save entire test data as npy file'''\n",
    "# df = pd.read_csv('test')\n",
    "# batcharr = df.values\n",
    "# print(batcharr.shape)\n",
    "# np.save('test_npy', df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(582967, 42)\n",
      "(318396, 42)\n",
      "(318203, 42)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load('train_reduced.npy')\n",
    "print(train_data.shape)\n",
    "val_data = np.load('val_reduced.npy')\n",
    "print(val_data.shape)\n",
    "test_data = np.load('test_reduced.npy')\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Put labels for classes : Normal = 0, DOS = 1, Probe = 2, R2L = 3, U2R = 4  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(Y):\n",
    "    Y_cat = np.ones(Y.shape)\n",
    "    Y_cat[Y == 'normal.'] = 0\n",
    "    DOS_types = ['back.', 'land.', 'neptune.', 'pod.', 'smurf.', 'teardrop.']\n",
    "    Probe_types = ['ipsweep.', 'nmap.', 'portsweep.', 'satan.']\n",
    "    R2L_types = ['ftp_write.', 'guess_passwd.', 'imap.', 'multihop.', 'phf.', 'spy.', 'warezclient.', 'warezmaster.']\n",
    "    U2R_types = ['buffer_overflow.', 'loadmodule.', 'perl.', 'rootkit.']\n",
    "    for index, i in enumerate(Y):\n",
    "        if i in DOS_types:\n",
    "            Y_cat[index] = 1 \n",
    "        if i in Probe_types:\n",
    "            Y_cat[index] = 2 \n",
    "        if i in R2L_types:\n",
    "            Y_cat[index] = 3\n",
    "        if i in U2R_types:\n",
    "            Y_cat[index] = 4 \n",
    "    return Y_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 582967 observations.\n"
     ]
    }
   ],
   "source": [
    "X = train_data[:,:-1]\n",
    "Y = train_data[:,-1]\n",
    "print(\"Training data has \"+ str(X.shape[0]) + \" observations.\")\n",
    "Y_new = get_labels(Y)\n",
    "X = np.delete(X,[1,2,3],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix as cmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999931385481511\n"
     ]
    }
   ],
   "source": [
    "model.fit(X,Y_new)\n",
    "\n",
    "print(model.score(X, Y_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation has 318396 observations.\n",
      "\n",
      "     precison    recall   f_score   support\n",
      "0.0  0.999734  0.999822  0.999778  213983.0\n",
      "1.0  0.999900  0.999910  0.999905   99676.0\n",
      "2.0  0.996384  0.996159  0.996272    4426.0\n",
      "3.0  0.954861  0.935374  0.945017     294.0\n",
      "4.0  0.500000  0.117647  0.190476      17.0\n",
      "precison        0.890176\n",
      "recall          0.809782\n",
      "f_score         0.826290\n",
      "support     63679.200000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "#                    validation                                 #\n",
    "#################################################################\n",
    "X_val = val_data[:,:-1]\n",
    "Y_val = val_data[:,-1]\n",
    "print(\"Validation has \"+str(X_val.shape[0])+\" observations.\")\n",
    "\n",
    "Y_val = get_labels(Y_val)\n",
    "X_val = np.delete(X_val,[1,2,3],1)\n",
    "Y_pr = model.predict(X_val)\n",
    "labels = np.unique(Y_val)\n",
    "\n",
    "prfsarr = np.array(prfs(Y_val, Y_pr, labels = labels))\n",
    "df = pd.DataFrame(prfsarr.T, columns = ['precison', 'recall', 'f_score', 'support'], index = labels).sort_values(by = 'support', ascending = False)\n",
    "print()\n",
    "print(df)\n",
    "print(df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data has 1224629 observations\n",
      "(array([0., 1., 2., 3., 4.]), array([243257, 970703,  10380,    283,      6]))\n",
      "(array([0., 1., 2., 3., 4.]), array([243250, 970708,  10361,    297,     13]))\n"
     ]
    }
   ],
   "source": [
    "test_data = np.load('test_npy.npy')\n",
    "print('Testing data has',str(test_data.shape[0]), 'observations')\n",
    "\n",
    "t_X = test_data[:,:-1]\n",
    "t_Y = test_data[:,-1]\n",
    "\n",
    "t_X_new = np.delete(t_X,[1,2,3],1)\n",
    "\n",
    "t_Y_predicted = model.predict(t_X_new)\n",
    "\n",
    "print(np.unique(t_Y_predicted, return_counts = True))\n",
    "\n",
    "t_Y = get_labels(t_Y)\n",
    "labels = np.unique(t_Y)\n",
    "print(np.unique(t_Y, return_counts = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     precison    recall   f_score   support\n",
      "0.0  0.999712  0.999741  0.999727  243250.0\n",
      "1.0  0.999985  0.999979  0.999982  970708.0\n",
      "2.0  0.996243  0.998070  0.997155   10361.0\n",
      "3.0  0.961131  0.915825  0.937931     297.0\n",
      "4.0  0.833333  0.384615  0.526316      13.0\n",
      "********************************************\n",
      "precison         0.958081\n",
      "recall           0.859646\n",
      "f_score          0.892222\n",
      "support     244925.800000\n",
      "dtype: float64\n",
      "********************************************\n",
      "Accuracy is 0.999888945958327\n"
     ]
    }
   ],
   "source": [
    "prfsarr = np.array(prfs(t_Y, t_Y_predicted, labels = labels))\n",
    "df = pd.DataFrame(prfsarr.T, columns = ['precison', 'recall', 'f_score', 'support'], index = labels)\n",
    "print(df)\n",
    "print(\"********************************************\")\n",
    "print(df.mean())\n",
    "print(\"********************************************\")\n",
    "accuracy = accuracy_score(t_Y, t_Y_predicted)\n",
    "print(\"Accuracy is \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 582967 observations.\n"
     ]
    }
   ],
   "source": [
    "X = train_data[:,:-1]\n",
    "Y = train_data[:,-1]\n",
    "print(\"Training data has \"+ str(X.shape[0]) + \" observations.\")\n",
    "Y_new = get_labels(Y)\n",
    "X = np.delete(X,[1,2,3],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999931385481511\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 300, random_state=0, class_weight='balanced')\n",
    "clf.fit(X,Y_new)\n",
    "print(clf.score(X, Y_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data has 318396 observations.\n",
      "\n",
      "     precison    recall   f_score   support\n",
      "0.0  0.999668  0.999953  0.999811  213983.0\n",
      "1.0  0.999960  0.999920  0.999940   99676.0\n",
      "2.0  0.999091  0.993448  0.996261    4426.0\n",
      "3.0  0.989130  0.928571  0.957895     294.0\n",
      "4.0  0.666667  0.117647  0.200000      17.0\n",
      "********************************************\n",
      "precison        0.930903\n",
      "recall          0.807908\n",
      "f_score         0.830781\n",
      "support     63679.200000\n",
      "dtype: float64\n",
      "********************************************\n",
      "Accuracy is 0.9997393183331449\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "#                    validation                                 #\n",
    "#################################################################\n",
    "X_val = val_data[:,:-1]\n",
    "Y_val = val_data[:,-1]\n",
    "print(\"Validation data has \"+str(X_val.shape[0])+\" observations.\")\n",
    "\n",
    "Y_val = get_labels(Y_val)\n",
    "X_val = np.delete(X_val,[1,2,3],1)\n",
    "\n",
    "Y_pr = clf.predict(X_val)\n",
    "labels = np.unique(Y_val)\n",
    "\n",
    "accuracy = accuracy_score(Y_val, Y_pr)\n",
    "prfsarr = np.array(prfs(Y_val, Y_pr, labels = labels))\n",
    "df = pd.DataFrame(prfsarr.T, columns = ['precison', 'recall', 'f_score', 'support'], index = labels).sort_values(by = 'support', ascending = False)\n",
    "print()\n",
    "print(df)\n",
    "print(\"********************************************\")\n",
    "print(df.mean())\n",
    "print(\"********************************************\")\n",
    "print(\"Accuracy is \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data has 1224629 observations\n",
      "(array([0., 1., 2., 3., 4.]), array([243283, 970702,  10363,    276,      5]))\n",
      "(array([0., 1., 2., 3., 4.]), array([243250, 970708,  10361,    297,     13]))\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "#                    Testing                                    #\n",
    "#################################################################\n",
    "test_data = np.load('test_npy.npy')\n",
    "print('Testing data has',str(test_data.shape[0]), 'observations')\n",
    "\n",
    "t_X = test_data[:,:-1]\n",
    "t_Y = test_data[:,-1]\n",
    "\n",
    "t_X_new = np.delete(t_X,[1,2,3],1)\n",
    "\n",
    "t_Y_predicted = clf.predict(t_X_new)\n",
    "\n",
    "\n",
    "print(np.unique(t_Y_predicted, return_counts = True))\n",
    "\n",
    "t_Y = get_labels(t_Y)\n",
    "labels = np.unique(t_Y)\n",
    "print(np.unique(t_Y, return_counts = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     precison    recall   f_score   support\n",
      "0.0  0.999745  0.999881  0.999813  243250.0\n",
      "1.0  0.999997  0.999991  0.999994  970708.0\n",
      "2.0  0.997491  0.997684  0.997587   10361.0\n",
      "3.0  1.000000  0.929293  0.963351     297.0\n",
      "4.0  1.000000  0.384615  0.555556      13.0\n",
      "********************************************\n",
      "precison         0.999447\n",
      "recall           0.862293\n",
      "f_score          0.903260\n",
      "support     244925.800000\n",
      "dtype: float64\n",
      "********************************************\n",
      "Accuracy is 0.9999256917809394\n"
     ]
    }
   ],
   "source": [
    "prfsarr = np.array(prfs(t_Y, t_Y_predicted, labels = labels))\n",
    "df = pd.DataFrame(prfsarr.T, columns = ['precison', 'recall', 'f_score', 'support'], index = labels)\n",
    "print(df)\n",
    "print(\"********************************************\")\n",
    "print(df.mean())\n",
    "print(\"********************************************\")\n",
    "accuracy = accuracy_score(t_Y, t_Y_predicted)\n",
    "print(\"Accuracy is \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 582967 observations.\n"
     ]
    }
   ],
   "source": [
    "X = train_data[:,:-1]\n",
    "Y = train_data[:,-1]\n",
    "print(\"Training data has \"+ str(X.shape[0]) + \" observations.\")\n",
    "Y_new = get_labels(Y)\n",
    "X = np.delete(X,[1,2,3],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 10)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999931385481511\n"
     ]
    }
   ],
   "source": [
    "clf2 = RandomForestClassifier(n_estimators = 300, random_state=0, class_weight='balanced')\n",
    "clf2.fit(X,Y_new)\n",
    "print(clf2.score(X, Y_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data has 318396 observations.\n",
      "\n",
      "     precison    recall   f_score   support\n",
      "0.0  0.999710  0.999916  0.999813  213983.0\n",
      "1.0  0.999709  0.999940  0.999824   99676.0\n",
      "2.0  0.996355  0.988251  0.992287    4426.0\n",
      "3.0  0.996377  0.935374  0.964912     294.0\n",
      "4.0  0.750000  0.176471  0.285714      17.0\n",
      "********************************************\n",
      "precison        0.94843\n",
      "recall          0.81999\n",
      "f_score         0.84851\n",
      "support     63679.20000\n",
      "dtype: float64\n",
      "********************************************\n",
      "Accuracy is 0.9996576590158168\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "#                    validation                                 #\n",
    "#################################################################\n",
    "val_data = np.load('val_reduced.npy')\n",
    "X_val = val_data[:,:-1]\n",
    "Y_val = val_data[:,-1]\n",
    "print(\"Validation data has \"+str(X_val.shape[0])+\" observations.\")\n",
    "\n",
    "Y_val = get_labels(Y_val)\n",
    "X_val = np.delete(X_val,[1,2,3],1)\n",
    "\n",
    "X_val = pca.transform(X_val)\n",
    "Y_pr = clf2.predict(X_val)\n",
    "labels = np.unique(Y_val)\n",
    "\n",
    "accuracy = accuracy_score(Y_val, Y_pr)\n",
    "prfsarr = np.array(prfs(Y_val, Y_pr, labels = labels))\n",
    "df = pd.DataFrame(prfsarr.T, columns = ['precison', 'recall', 'f_score', 'support'], index = labels).sort_values(by = 'support', ascending = False)\n",
    "print()\n",
    "print(df)\n",
    "print(\"********************************************\")\n",
    "print(df.mean())\n",
    "print(\"********************************************\")\n",
    "print(\"Accuracy is \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data has 1224629 observations\n",
      "(array([0., 1., 2., 3., 4.]), array([243289, 970726,  10337,    271,      6]))\n",
      "(array([0., 1., 2., 3., 4.]), array([243250, 970708,  10361,    297,     13]))\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "#                    Testing                                    #\n",
    "#################################################################\n",
    "test_data = np.load('test_npy.npy')\n",
    "print('Testing data has',str(test_data.shape[0]), 'observations')\n",
    "\n",
    "t_X = test_data[:,:-1]\n",
    "t_Y = test_data[:,-1]\n",
    "\n",
    "t_X_new = np.delete(t_X,[1,2,3],1)\n",
    "\n",
    "t_X_new = pca.transform(t_X_new)\n",
    "t_Y_predicted = clf2.predict(t_X_new)\n",
    "\n",
    "print(np.unique(t_Y_predicted, return_counts = True))\n",
    "\n",
    "t_Y = get_labels(t_Y)\n",
    "labels = np.unique(t_Y)\n",
    "print(np.unique(t_Y, return_counts = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     precison    recall   f_score   support\n",
      "0.0  0.999679  0.999840  0.999760  243250.0\n",
      "1.0  0.999968  0.999987  0.999977  970708.0\n",
      "2.0  0.996034  0.993726  0.994879   10361.0\n",
      "3.0  1.000000  0.912458  0.954225     297.0\n",
      "4.0  1.000000  0.461538  0.631579      13.0\n",
      "********************************************\n",
      "precison         0.999136\n",
      "recall           0.873510\n",
      "f_score          0.916084\n",
      "support     244925.800000\n",
      "dtype: float64\n",
      "********************************************\n",
      "Accuracy is 0.9998775139246253\n"
     ]
    }
   ],
   "source": [
    "prfsarr = np.array(prfs(t_Y, t_Y_predicted, labels = labels))\n",
    "df = pd.DataFrame(prfsarr.T, columns = ['precison', 'recall', 'f_score', 'support'], index = labels)\n",
    "print(df)\n",
    "print(\"********************************************\")\n",
    "print(df.mean())\n",
    "print(\"********************************************\")\n",
    "accuracy = accuracy_score(t_Y, t_Y_predicted)\n",
    "print(\"Accuracy is \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGboost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 582967 observations.\n",
      "[0. 1. 2. 3. 4.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=8, min_child_weight=1, missing=None,\n",
       "       n_estimators=150, n_jobs=1, nthread=None, num_class=5,\n",
       "       objective='multi:softprob', random_state=1, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_data[:,:-1]\n",
    "Y = train_data[:,-1]\n",
    "print(\"Training data has \"+ str(X.shape[0]) + \" observations.\")\n",
    "Y_new = get_labels(Y)\n",
    "print(np.unique(Y_new))\n",
    "X = np.delete(X,[1,2,3],1)\n",
    "\n",
    "\n",
    "# xgb = xgboost.XGBClassifier(objective ='reg:logistic',n_estimators=150, random_state=1,learning_rate=0.01)\n",
    "xgb = xgboost.XGBClassifier(objective ='multi:softmax', num_class = 5, n_estimators = 150,\n",
    "                            max_depth = 8, random_state=1, learning_rate=0.1)\n",
    "\n",
    "xgb.fit(X, Y_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data has 318396 observations.\n",
      "\n",
      "     precison    recall   f_score   support\n",
      "0.0  0.999780  0.999953  0.999867  213983.0\n",
      "1.0  0.999970  0.999940  0.999955   99676.0\n",
      "2.0  0.998416  0.996611  0.997512    4426.0\n",
      "3.0  0.992883  0.948980  0.970435     294.0\n",
      "4.0  0.750000  0.176471  0.285714      17.0\n",
      "********************************************\n",
      "precison        0.948210\n",
      "recall          0.824391\n",
      "f_score         0.850697\n",
      "support     63679.200000\n",
      "dtype: float64\n",
      "********************************************\n",
      "Accuracy is 0.9998115554215505\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "#                    validation                                 #\n",
    "#################################################################\n",
    "val_data = np.load('val_reduced.npy')\n",
    "X_val = val_data[:,:-1]\n",
    "Y_val = val_data[:,-1]\n",
    "print(\"Validation data has \"+str(X_val.shape[0])+\" observations.\")\n",
    "\n",
    "Y_val = get_labels(Y_val)\n",
    "X_val = np.delete(X_val,[1,2,3],1)\n",
    "\n",
    "Y_pr = xgb.predict(X_val)\n",
    "labels = np.unique(Y_val)\n",
    "\n",
    "accuracy = accuracy_score(Y_val, Y_pr)\n",
    "prfsarr = np.array(prfs(Y_val, Y_pr, labels = labels))\n",
    "df = pd.DataFrame(prfsarr.T, columns = ['precison', 'recall', 'f_score', 'support'], index = labels).sort_values(by = 'support', ascending = False)\n",
    "print()\n",
    "print(df)\n",
    "print(\"********************************************\")\n",
    "print(df.mean())\n",
    "print(\"********************************************\")\n",
    "print(\"Accuracy is \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data has 1224629 observations\n",
      "(array([0., 1., 2., 3., 4.]), array([243282, 970703,  10351,    287,      6]))\n",
      "(array([0., 1., 2., 3., 4.]), array([243250, 970708,  10361,    297,     13]))\n",
      "     precison    recall   f_score   support\n",
      "0.0  0.999803  0.999934  0.999868  243250.0\n",
      "1.0  0.999995  0.999990  0.999992  970708.0\n",
      "2.0  0.998841  0.997877  0.998358   10361.0\n",
      "3.0  0.996516  0.962963  0.979452     297.0\n",
      "4.0  1.000000  0.461538  0.631579      13.0\n",
      "********************************************\n",
      "precison         0.999031\n",
      "recall           0.884460\n",
      "f_score          0.921850\n",
      "support     244925.800000\n",
      "dtype: float64\n",
      "********************************************\n",
      "Accuracy is 0.9999461061268351\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "#                    Testing                                    #\n",
    "#################################################################\n",
    "test_data = np.load('test_npy.npy')\n",
    "print('Testing data has',str(test_data.shape[0]), 'observations')\n",
    "\n",
    "t_X = test_data[:,:-1]\n",
    "t_Y = test_data[:,-1]\n",
    "\n",
    "t_X_new = np.delete(t_X,[1,2,3],1)\n",
    "\n",
    "t_Y_predicted = xgb.predict(t_X_new)\n",
    "\n",
    "\n",
    "print(np.unique(t_Y_predicted, return_counts = True))\n",
    "\n",
    "t_Y = get_labels(t_Y)\n",
    "labels = np.unique(t_Y)\n",
    "\n",
    "print(np.unique(t_Y, return_counts = True))\n",
    "prfsarr = np.array(prfs(t_Y, t_Y_predicted, labels = labels))\n",
    "df = pd.DataFrame(prfsarr.T, columns = ['precison', 'recall', 'f_score', 'support'], index = labels)\n",
    "print(df)\n",
    "print(\"********************************************\")\n",
    "print(df.mean())\n",
    "print(\"********************************************\")\n",
    "accuracy = accuracy_score(t_Y, t_Y_predicted)\n",
    "print(\"Accuracy is \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
